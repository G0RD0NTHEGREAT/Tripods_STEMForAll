{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Two: Vapnik–Chervonenkis dimension\n",
    "In this project, we will explore the concept of VC dimension in a series of learning examples.\n",
    "\n",
    "The goal of the project is to understand\n",
    "\n",
    "1) how knowledge about the VC dimension of a hypothesis class plays a role in determining the sample complexity needed to ensure probably approximately correct (PAC) learning under the realizability assumption and,\n",
    "\n",
    "2) conversely, what the sample complexity for PAC learning implies about the VC dimension of hypothesis class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background review:\n",
    "\n",
    "We first recall some important definitions in this setting. All page numbers refer to Understanding Machine Learning: From Theory to Algorithms by Shai Shalev-Shwartz and Shai Ben-David. The material covered here is primarily from chapters 6 and 9.\n",
    "\n",
    "#### Definition 1 (Restriction of $\\mathcal{H}$ to $C$):\n",
    "Let $\\mathcal{H}$ be a class of functions from $X$ to $\\{0, 1\\}$ and let $C = \\{c_1, . . . , c_m\\} \\subseteq X.$ The restriction of $\\mathcal{H}$ to $C$ is the set of functions from $C$ to $\\{0, 1\\}$ that can be derived from $\\mathcal{H}$. That is,\n",
    "$$\\mathcal{H}_C = \\{(h(c_1), . . . ,h(c_m)) : h \\in H\\},$$\n",
    "where we represent each function from $C$ to $\\{0, 1\\}$ as a vector in $\\{0, 1\\}^{|C|}$.\n",
    "\n",
    "#### Definition 2 (Shattering): \n",
    "A hypothesis class $\\mathcal{H}$ shatters a finite set $C \\rightarrow X $ if the restriction of $\\mathcal{H}$ to $C$ is the set of all functions from $C$ to $\\{0, 1\\}$. That is, $|\\mathcal{H}_C| = 2^{|C|}.$\n",
    "\n",
    "#### Defintion 3 (VC dimension): \n",
    "The VC-dimension of a hypothesis class $\\mathcal{H}$,denoted $VCdim(\\mathcal{H})$, is the maximal size of a set $C \\subseteq X$ that can be shattered by $\\mathcal{H}$. If $\\mathcal{H}$ can shatter sets of arbitrarily large size we say that $\\mathcal{H}$ has infinite VC-dimension.\n",
    "\n",
    "A fundamental theorem that will be in use is the following which indicates the relation between the VC dimension and PAC learnablity:\n",
    "\n",
    "#### Theorem 1 (The Fundamental Theorem of Statistical Learning):\n",
    "Let $\\mathcal{H}$ be a hypothesis class of functions from a domain $X$ to $\\{0, 1\\}$ and let the loss function be the $0 − 1$ loss. Then, the following are equivalent:\n",
    "1. $\\mathcal{H}$ has the uniform convergence property.\n",
    "2. Any ERM rule is a successful agnostic PAC learner for $\\mathcal{H}$.\n",
    "3. $\\mathcal{H}$ is agnostic PAC learnable.\n",
    "4. $\\mathcal{H}$ is PAC learnable.\n",
    "5. Any ERM rule is a successful PAC learner for $\\mathcal{H}$.\n",
    "6. $\\mathcal{H}$ has a finite VC-dimension.\n",
    "\n",
    "The uniform convergence property of a hypothesis class with finite VC-dimension can be quantified further which will play an important role in this project:\n",
    "\n",
    "#### Theorem 2 (The Fundamental Theorem of Statistical Learning – Quantitative Version):\n",
    "Let $\\mathcal{H}$ be a hypothesis class of functions from a domain $X$ to $\\{0, 1\\}$\n",
    "and let the loss function be the $0 − 1$ loss. Assume that $VCdim(H) = d < \\infty.$\n",
    "Then, there are absolute constants $C_1, C_2$ such that: \n",
    "1. $\\mathcal{H}$ has the uniform convergence property with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon^2}\\le m_{\\mathcal{H}}^{UC}(\\epsilon,\\delta)\\le C_2\\frac{d+\\log(1/\\delta)}{\\epsilon^2}$$\n",
    "2. $\\mathcal{H}$ is agnostic PAC learnable with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon^2}\\le m_{\\mathcal{H}}(\\epsilon,\\delta)\\le C_2\\frac{d+\\log(1/\\delta)}{\\epsilon^2}$$\n",
    "3. $\\mathcal{H}$ is PAC learnable with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon}\\le m_{\\mathcal{H}}(\\epsilon,\\delta)\\le C_2\\frac{d\\log(1/\\epsilon)+\\log(1/\\delta)}{\\epsilon}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: For all the examples below, we use the $0-1$ loss and we assume the realizabilty condition.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 1:\n",
    "Let $X=\\{1,2,\\ldots,2^{20}-1\\}$ and let $\\mathcal{H}$ be the class of threshold functions $\\{x\\mapsto\\mathbb{1}_{x\\leq a}:a\\in\\mathbb{R}\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questions\n",
    "1) Given the finiteness of the hypothesis class, what can you say immediately about the upper bound for the VC dimesion of $\\mathcal{H}$?\n",
    "\n",
    "Answer: As per pg.72 of the text, we always have the bound $\\operatorname{VCdim}(\\mathcal{H})\\le\\log_2(|\\mathcal{H}|)$. In this case we see that $\\operatorname{VCdim}(\\mathcal{H})\\le\\log_2(2^{20})=20$.\n",
    "\n",
    "2) Can you improve the trivial bound and find the exact VC dimension of $\\mathcal{H}$?\n",
    "\n",
    "Answer: We have that $\\operatorname{VCdim}(\\mathcal{H})=1$ as discussed on pg.70.\n",
    "\n",
    "3) Based on the knowledge of VC dimension of $\\mathcal{H}$, how many samples do you need to draw to ensure that the output hypothesis is probably approximately accurate with 98% accuracy and 95% confidence?\n",
    "\n",
    "Answer: The Fundamental Theorem of Statistical Learning (Quantitative Version) says that $\\mathcal{H}$ is is PAC learnable with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon}\\le m_{\\mathcal{H}}(\\epsilon,\\delta)\\le C_2\\frac{d\\log(1/\\epsilon)+\\log(1/\\delta)}{\\epsilon}.$$ In this case, the confidence parameter is $\\delta=0.05$, the accuracy parameter is $\\epsilon=0.02$, and $d=\\operatorname{VCdim}(\\mathcal{H})=1$ so we have that there exist absolute constants $C_1$ and $C_2$ such that $$C_1\\frac{1+\\log(1/0.05)}{0.02}\\le m_{\\mathcal{H}}(\\epsilon,\\delta)\\le C_2\\frac{1\\log(1/0.02)+\\log(1/0.05)}{0.02}.$$ Here we can take $C_2=1200$, giving an explicit upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small example:\n",
    "We give a small example of using the algorithm described in Example 6.1 on pg.67."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As is often the case, we will use numpy.\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300.0\n"
     ]
    }
   ],
   "source": [
    "# We are given a list of pairs of the form (x,1) or (x,-1) for x in the appropriate range.\n",
    "S = np.array([[1,1],[-10,1],[100,1],[1000,-1],[500,-1]])\n",
    "\n",
    "# Find the smallest input in the sample such that the label is -1 and the largest input such that the label is 1:\n",
    "# Choose initial bounds S_left and S_right, which are negative and positive infinity.\n",
    "# (These are implemented in numpy.)\n",
    "S_left = -np.Inf\n",
    "S_right = np.Inf\n",
    "\n",
    "# For all entries (x,y) in S\n",
    "size = 5\n",
    "for i in range(size):\n",
    "    # If x is bigger than the lower bound S_left\n",
    "    if S[i,0] > S_left:\n",
    "        # If the value y is 1 make x the new S_left\n",
    "        if S[i,1] == 1:\n",
    "            S_left = S[i,0]\n",
    "        # If the value y is -1 make and x is smaller than S_right, make x the new S_right\n",
    "        else:\n",
    "            if S[i,0] < S_right:\n",
    "                S_right = S[i,0]\n",
    "\n",
    "# The learned threshold function is the indicator function for values up to the average of S_left and S_right.\n",
    "threshold = (S_left + S_right)/2\n",
    "print(threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 2: \n",
    "Let $\\mathcal{H}$ be the class of (nonhomogeneous) linear classifiers over $\\mathbb{R}^d$.\n",
    "\n",
    "#### Questions:\n",
    "1) What is the VC dimension of $\\mathcal{H}$?\n",
    "\n",
    "Answer: Theorem 9.3 on pg.122 states that $\\operatorname{VCdim}(\\mathcal{H})=d+1$.\n",
    "\n",
    "2) How many samples do you need to draw to ensure that the output hypothesis is probably approximately accurate with 98% accuracy and 95% confidence?\n",
    "\n",
    "Answer: The Fundamental Theorem of Statistical Learning (Quantitative Version) says that $\\mathcal{H}$ is is PAC learnable with sample complexity $$C_1\\frac{d+\\log(1/\\delta)}{\\epsilon}\\le m_{\\mathcal{H}}(\\epsilon,\\delta)\\le C_2\\frac{d\\log(1/\\epsilon)+\\log(1/\\delta)}{\\epsilon}.$$ In this case, the confidence parameter is $\\delta=0.05$, the accuracy parameter is $\\epsilon=0.02$, and the VC dimension of $\\mathcal{H}$ is $d+1$ so we have that there exist absolute constants $C_1$ and $C_2$ such that $$C_1\\frac{d+1+\\log(1/0.05)}{0.02}\\le m_{\\mathcal{H}}(\\epsilon,\\delta)\\le C_2\\frac{(d+1)\\log(1/0.02)+\\log(1/0.05)}{0.02}.$$ Here we can take $C_2=1200$, giving an explicit upper bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Small example:\n",
    "We give a small example of using halfspaces and linear programming to classify points in $\\mathbb{R}^2$. That is, we will use the algorithm given in section 9.1.1 on pg.119 to choose a hypothesis $h$ from the class $\\mathcal{H}$ so that the ERM rule is satisfied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     con: array([], dtype=float64)\n",
      "     fun: 0.0\n",
      " message: 'Optimization terminated successfully.'\n",
      "     nit: 4\n",
      "   slack: array([1.29164073, 1.0358591 , 1.54742235])\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([-2.12111965,  2.20638019])\n"
     ]
    }
   ],
   "source": [
    "# We use the linear program solver from scipy for this purpose.\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Here we have a collection of 3 points in the plane.\n",
    "# Those points are (1,2), (2,1), and (4,5).\n",
    "# The last coordinate in each triple is the label.\n",
    "S = np.array([[1,2,1],[2,1,-1],[4,5,1]])\n",
    "size = 3\n",
    "\n",
    "# Rreorganize the sample to a form that is usable for linear programming:\n",
    "# Create a vector u consisting of as many zeroes as there are coordinates in our points.\n",
    "u = np.zeros(2)\n",
    "# Create an array S_x which consists of only the points, not their labels.\n",
    "S_x = S[0:size,0:2]\n",
    "# Make an array S_label which consists of only the labels, not their points. Hint: np.reshape is your friend.\n",
    "S_label = np.reshape(S[0:size,2],(3,1))\n",
    "# Multiply S_x and S_label.\n",
    "# This negates the appropriate rows so that linear programming will max/minimize as needed.\n",
    "A = S_x * S_label\n",
    "# Negate the resulting array or else the learned function will have labels swapped.\n",
    "B = -A\n",
    "# Make an array consisting of all -1 whose length is the number of coordinates in a point.\n",
    "v = -np.ones(size)\n",
    "# Use linprog to solve the linear program and display the solution.\n",
    "w = linprog(u, A_ub=B, b_ub=v, bounds=((None, None),(None, None)))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.291640727123371, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test the linear classifier so obtained.\n",
    "# It's given as an array, say w, whose length is the number of coordinates in a point.\n",
    "# Test it on values by taking the dot product with a point and seeing whether it's positive.\n",
    "np.dot(w.x,S[0,0:2]), S[0,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-world example:\n",
    "We will now use linear classifiers to learn from a real-world dataset. The dataset we will use is the new industry standard Fashion MNIST described on this TensorFlow tutorial: https://www.tensorflow.org/tutorials/keras/classification. Unfortunately, TensorFlow doesn't seem to play nice with Juptyer at the moment, so we can't use it to import the data set. Instead we will obtain premade CSV files from GitHub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will get our training data from https://github.com/silky/fashion-mnist-csv\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('https://media.githubusercontent.com/media/silky/fashion-mnist-csv/master/mnist_train.csv',header=None)\n",
    "test_data = pd.read_csv('https://media.githubusercontent.com/media/silky/fashion-mnist-csv/master/mnist_test.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the data.columns method to relabel your data with the first column 'Label' and the latter columns\n",
    "# (1,1) through (28,28), which represent the pixels in a 28 by 28 greyscale image of a clothing article.\n",
    "train_data.columns = ['Label']+[(i,j) for i in range(1,29) for j in range(1,29)]\n",
    "test_data.columns = ['Label']+[(i,j) for i in range(1,29) for j in range(1,29)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the training data.\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine the test data.\n",
    "test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the tops (0) and pants (1) labels using the data.loc method for both datasets.\n",
    "train_data = train_data.loc[(train_data['Label']==0) | (train_data['Label']==1)]\n",
    "test_data = test_data.loc[(test_data['Label']==0) | (test_data['Label']==1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to a numpy array.\n",
    "# Change the zero labels to -1 so that the same linear solver technique from before will work.\n",
    "train_np = train_data.to_numpy()\n",
    "for i in range(len(train_np)):\n",
    "    if train_np[i][0] == 0:\n",
    "        train_np[i][0] = -1\n",
    "test_np = test_data.to_numpy()\n",
    "for i in range(len(test_np)):\n",
    "    if test_np[i][0] == 0:\n",
    "        test_np[i][0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a small number of samples from the training dataset, say 500, and use the linear solver on them.\n",
    "# Now our points are in a 28**2 dimensional space instead of the 2 dimensional plane.\n",
    "size = 500\n",
    "assert size<len(train_np)\n",
    "u = np.zeros(28**2)\n",
    "# S_x = np.array(dtype=np.float32)\n",
    "S_x = train_np[0:size,1:28**2+1]\n",
    "S_label = np.reshape(train_np[0:size,0],(size,1))\n",
    "A = S_x * S_label\n",
    "B = -A \n",
    "v = -np.ones(size)\n",
    "w = linprog(u, A_ub=B, b_ub=v, bounds=tuple((None,None) for i in range(28**2)))\n",
    "print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the fraction of successes you have on the test set.\n",
    "# That is, how often does the dot product of your solution with an entry in the test set yield a number\n",
    "# with the same sign as its label?\n",
    "successes = 0\n",
    "for i in range(size):\n",
    "    if np.dot(w.x,test_np[i,1:28**2+1])*test_np[i,0]>0:\n",
    "        successes += 1\n",
    "print(successes/size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example 3: \n",
    "\n",
    "Let $\\mathcal{H}$ be a class of functions over $\\mathbb{R}$ defined by $\\{x \\mapsto \\lceil{\\sin(\\theta x)}\\rceil: \\theta \\in \\mathbb{R}\\}$. Please learn the samples drawn from the data set using the ERM rule.  \n",
    "\n",
    "#### Questions:\n",
    "1) Can you answer the same questions posed in Example 2? If yes, how does it explain the learning experiments and their accuracy? If not, could you guess from the experiments on the VC-dimension of $\\mathcal{H}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hint: Use the hint in the textbook Chapter 8/Exercise 8. In particular, if $0.x_1x_2x_3 . . .$ is the binary expansion of $x \\in (0,1)$, then for any natural number $m$, $\\lceil\\sin(2^m \\pi x)\\rceil = (1−x_m)$, provided\n",
    "that there exist $k \\geq m$ s.t. $x_k = 1$.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a sample from the data set:\n",
    "\n",
    "# Design a function which convert a float to binary expansion with the first 1000 places:\n",
    "\n",
    "\n",
    "# Rearrange the data with input x being a binary expansion and the output the label of 0 or 1:\n",
    "S_x = np.array([[0,0,1,0,0,1],[0,0,0,0,1,0], [1,1,1,1,0,0]])\n",
    "y = np.array([[0],[1],[0]])\n",
    "#S = np.hstack((S_x,y))\n",
    "size = 3\n",
    "binarylen = 6\n",
    "\n",
    "# Let y be the label corresponding to x. Find the indices m of the array x satisfying 1-x_m = y:\n",
    "ind = np.zeros((size,binarylen))\n",
    "for i in range(size):\n",
    "    for j in range(binarylen):\n",
    "        if 1-S_x[i,j] == y[i]:\n",
    "            ind[i,j] = 1\n",
    "\n",
    "# Find the smallest common index m such that 1-x_m = y for all (x,y) in the sample:\n",
    "m = None\n",
    "i = 0\n",
    "j = 0\n",
    "while j < binarylen and m == None:\n",
    "    while i < size and ind[i,j] == 1:\n",
    "        i += 1\n",
    "    if i == size:\n",
    "        m = j\n",
    "    j += 1\n",
    "    i = 0\n",
    "\n",
    "print(ind)\n",
    "print(m)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
