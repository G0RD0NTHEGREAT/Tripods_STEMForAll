{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uu6Vt-728Vq-"
   },
   "source": [
    "# Neural Networks and Gradient Descent\n",
    "\n",
    "\n",
    "In this project, we will be implementing a neural network and train it from scratch. In other words, we will temporarily put aside the wheels that others have built (e.g., TensorFlow, PyTorch, Keras, MXNet, etc) and take closer look at the mechamisms of neural networks inside those wrappers (but of course, we would greatly benefit from the linear algebra implementations of, say NumPy).\n",
    "\n",
    "## Neural Networks\n",
    "\n",
    "Recall that a neural network is composed of an input layer, an output layer, and an arbitrary number of hidden layers. \n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1000/1*g9agaYlewb0vzuJIOupf3w.png\" height=400 />\n",
    "\n",
    "For example, if we have a\n",
    "$$\n",
    "\\begin{align*}\n",
    "    z & = \\sigma(W_1x) \\\\\n",
    "    y & = \\sigma(W_2z) = \\sigma(W_2\\sigma(W_1x))\n",
    "\\end{align*}\n",
    "$$\n",
    "where $\\sigma$ is the [sigmoid function](https://en.wikipedia.org/wiki/Sigmoid_function).\n",
    "\n",
    "One technical caveat is that we can omit the bias term as long as we append an $1$ at the end of $x$, which can somewhat simplify the backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VTWIwG0e8KKE",
    "outputId": "9eec45de-0cbc-4ae3-a8fb-d81a2cc49bef"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import math as math\n",
    "\n",
    "# useful activation function\n",
    "def sigmoid(x):\n",
    "    # TODO please skim the Wikipedia page for the sigmoid function, which is \n",
    "    # hyperlinked above and implmenet the sigmoid function\n",
    "    output = (1/(1+math.exp(-x)))\n",
    "    return(output)\n",
    "\n",
    "# test\n",
    "sigmoid(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgfxoqWV6-qS"
   },
   "source": [
    "Exercise: what is the derivative of sigmoid? Is it easy to compute numerically?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EvVLGRUxnGTs"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# global parameters\n",
    "INPUT_DIMENSION = 1024\n",
    "OUTPUT_DIMENSION = 1\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_dimension):\n",
    "        # naive random weight initialization\n",
    "        self.w1 = np.random.rand(hidden_dimension, INPUT_DIMENSION)\n",
    "        self.w2 = np.random.rand(OUTPUT_dimension, hidden_dimension)\n",
    "\n",
    "def forward(nn, x):\n",
    "    # TODO please implement the forward pass\n",
    "    # try to write it without any for loop\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jQ5uVpdzxJcJ"
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Gradient descent is a way of finding (with some error) the optimal weights with respect to some loss function. It is not guaranteed to produce THE optimal weights (in fact, it almost never produce them in practical applications) but it is extremely effective for optimizing machine learning models like neural networks.\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/700px-Gradient_descent.svg.png\" height=400>\n",
    "\n",
    "Algorithmically speaking, gradient descent is simply repetitively setting\n",
    "\n",
    "$$\n",
    "w = w - \\eta \\frac{\\partial L}{\\partial w}\n",
    "$$\n",
    "\n",
    "given some loss function $L$ and some positive number $\\eta$, a.k.a., the learning rate.\n",
    "\n",
    "Commonly used loss functions include $L_1$ [loss](https://pytorch.org/docs/stable/generated/torch.nn.L1Loss.html?highlight=loss#torch.nn.L1Loss \"PyTorch documentation on L1 loss\"), $L_2$ [loss](https://pytorch.org/docs/stable/generated/torch.nn.MSELoss.html \"PyTorch documentation on related MSE loss\"), and <a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html?highlight=bce#torch.nn.BCELoss\" title=\"PyTorch documentation on BCE loss\">cross entropy loss</a>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zVDCPA_5Fzp"
   },
   "source": [
    "## Back Propogation\n",
    "\n",
    "Gradient descent is simple yet powerful. But to actually implement it, we still need to find out $\\frac{\\partial L}{\\partial w}$. To do this, we use a method called back propogation (BP). In essense, BP is just a series of chain rules to propate the loss from the output layer back to the input layer.\n",
    "\n",
    "Take the previous neural network as an example, we have\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\frac{\\partial L}{\\partial W_2} & = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial W_2} = \\frac{\\partial L}{\\partial y} \\frac{\\partial y}{\\partial (W_2z)} \\frac{\\partial (W_2z)}{\\partial W_2} \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "where all the terms are easy to compute.\n",
    "\n",
    "Note that if we do bookkeeping properly, one single backward pass is enough to update all the weights.\n",
    "\n",
    "Exercise: derive a similar formula for $\\frac{\\partial L}{\\partial W_1}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6OL7kx-uHi0B"
   },
   "source": [
    "## Prepare your dataset\n",
    "\n",
    "In this project, we will continue using the fashion MNIST dataset or the MNIST dataset from the previous project. However, please feel free to use your favorite dataset if you want. For example,\n",
    "\n",
    "- [Iris](https://archive.ics.uci.edu/ml/datasets/Iris/) dataset\n",
    "- [Heart Attack](https://www.kaggle.com/rashikrahmanpritom/heart-attack-analysis-prediction-dataset?select=heart.csv) dataset or any other numerical dataset on Kaggle with a target variable (Kaggle account required for download but it's free)\n",
    "\n",
    "If the download link does not work, you can also download the dataset from [here](https://www.kaggle.com/zalando-research/fashionmnist \"Fashion MNIST dataset on Kaggle\") with a Kaggle account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0cI60kXCsPwp"
   },
   "outputs": [],
   "source": [
    "# Load the fashion MNIST dataset and perform preprocessing as in the last project\n",
    "import pandas as pd\n",
    "train_data = pd.read_csv('PATH_TO_TRAIN_DATA')\n",
    "test_data = pd.read_csv('PATH_TO_TRAIN_DATA')\n",
    "\n",
    "# Keep only the tops (0) and pants (1) labels using the data.loc method for both datasets.\n",
    "train_data = train_data.loc[(train_data['label']==0) | (train_data['label']==1)]\n",
    "test_data = test_data.loc[(test_data['label']==0) | (test_data['label']==1)]\n",
    "\n",
    "# Convert the data to a numpy array.\n",
    "# Change the zero labels to -1 so that the same linear solver technique from before will work.\n",
    "train_np = train_data.to_numpy()\n",
    "for i in range(len(train_np)):\n",
    "    if train_np[i][0] == 0:\n",
    "        train_np[i][0] = -1\n",
    "test_np = test_data.to_numpy()\n",
    "for i in range(len(test_np)):\n",
    "    if test_np[i][0] == 0:\n",
    "        test_np[i][0] = -1\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "train_images = train_np[:,1:].reshape(12000,28,28)\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(train_images[i])\n",
    "    plt.xlabel(train_np[i,0])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ij5t-PTGoBN6"
   },
   "outputs": [],
   "source": [
    "# backprop parameters\n",
    "MAX_EPOCH = 100\n",
    "data = []\n",
    "lr = 1e-3\n",
    "\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, hidden_dimension):\n",
    "        # naive random weight initialization\n",
    "        self.w1 = np.random.rand(hidden_dimension, INPUT_DIMENSION)\n",
    "        self.w2 = np.random.rand(OUTPUT_dimension, hidden_dimension)\n",
    "\n",
    "def backward_propogation(self, train_xs, train_ys, alpha):\n",
    "        # First, we need to do a forward propogation while keeping track of \n",
    "        # the hidden activations because we need them for back propogation.\n",
    "        # You might also decide to append a 1 to the end of x as the bias term\n",
    "        # to make back prop easier\n",
    "\n",
    "        # Calculate the derivative of the loss w.r.t y with the loss of your choice\n",
    "        dL_dy = \n",
    "        \n",
    "        # The main course\n",
    "        # Calculate the derivative of the loss w.r.t. the product of W_2 and z\n",
    "        # (the input of the activation function of the last layer)\n",
    "        delta_1 = \n",
    "\n",
    "        # Calculate the derivative of the loss w.r.t. W_2\n",
    "\n",
    "\n",
    "        # Calculate the derivative of the loss w.r.t z\n",
    "\n",
    "\n",
    "        # Calculate the derivative of the loss w.r.t the product of W_1 and x\n",
    "        # (the input of the activation function of the hidden layer)\n",
    "        delta_2 = \n",
    "\n",
    "        # Calculate the derivative fo the loss w.r.t x\n",
    "        \n",
    "        # update weights\n",
    "        self.w2 -= lr * \n",
    "        self.w1 -= lr * \n",
    "def train_model(model, train_ys, train_xs, dev_ys, dev_xs, args):\n",
    "    training_data = [(train_xs[n], train_ys[n]) for n in range(len(train_xs))]\n",
    "    for _ in range(MAX_EPOCH):\n",
    "        for (x, y) in training_data:\n",
    "            # back propogate\n",
    "            backward_propogation(model, x, y, args.lr)\n",
    "        # calculate and print the loss\n",
    "        train_y_hat = model.forward_propogation(train_xs)\n",
    "        loss = -np.sum(np.multiply(train_ys, np.log(train_y_hat))) - np.sum(np.multiply(1-train_ys, np.log(1-train_y_hat)))\n",
    "        print(\"Total training loss: \" + str(loss))\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tripods_StemForAll_Proj_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
